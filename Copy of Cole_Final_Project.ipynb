{"cells":[{"cell_type":"code","source":["# (c) Westerveld 2023\n","# Watermark remover training and development using pytorch library with python\n","# A convolutional variational autoencoder that takes in an image with a watermark and (ideally) outputs the same image but without a watermark\n","# The architecture compresses the image then upsamples it, removing the watermark in the process and outputting an image.\n","# uses CLWD dataset"],"metadata":{"id":"myKQ0k5vljft"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230953,"status":"ok","timestamp":1686764472276,"user":{"displayName":"FlyPengiun 44","userId":"12676730952236288290"},"user_tz":240},"id":"2VKm0IHvHKa7","outputId":"5f66bb17-5def-48d4-8ad2-5f3167d43d30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=17y1gkUhIV6rZJg1gMG-gzVMnH27fm4Ij\n","To: /content/CLWD.rar\n","100% 3.35G/3.35G [00:48<00:00, 69.0MB/s]\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyunpack\n","  Downloading pyunpack-0.3-py2.py3-none-any.whl (4.1 kB)\n","Collecting easyprocess (from pyunpack)\n","  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n","Collecting entrypoint2 (from pyunpack)\n","  Downloading entrypoint2-1.1-py2.py3-none-any.whl (9.9 kB)\n","Installing collected packages: entrypoint2, easyprocess, pyunpack\n","Successfully installed easyprocess-1.1 entrypoint2-1.1 pyunpack-0.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting patool\n","  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: patool\n","Successfully installed patool-1.12\n"]}],"source":["!gdown 17y1gkUhIV6rZJg1gMG-gzVMnH27fm4Ij\n","\n","!pip install pyunpack\n","!pip install patool\n","from pyunpack import Archive\n","Archive(\"CLWD.rar\").extractall(\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4055,"status":"ok","timestamp":1686764219717,"user":{"displayName":"FlyPengiun 44","userId":"12676730952236288290"},"user_tz":240},"id":"OpICVzD8XzXG","outputId":"64df63a1-8e8a-492f-db39-4983079b42fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# include libraries\n","#!pip install torchvision\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.io import read_image\n","from PIL import Image\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import torchvision.transforms as T\n","import random\n","import torch.nn.functional as F\n","\n","# check if gpu available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_hHozUR8fTx"},"outputs":[],"source":["# custom dataset - length 60 000, returns watermarked and watermark free image\n","class Coles(Dataset):\n","  def __init__(self, watermark_free, watermarked, transform = None, target_transform = None):\n","    #self.watermark_free = Image.open(\"CLWD/train/Watermark_image\" + watermark_free)\n","    #self.img_dir = watermarked\n","    self.transform = transform\n","    self.target_transform = target_transform\n","  # C:\\Users\\Cole\\Desktop/CLWD/\n","  def __getitem__(self, idx):\n","    img_path = r\"C:\\Users\\Cole\\Desktop/CLWD/CLWD/train/Watermarked_image/\" + str(idx + 1) + \".jpg\"\n","    watermarked = Image.open(img_path).convert('RGB')\n","    img_path = r\"C:\\Users\\Cole\\Desktop/CLWD/CLWD/train/Watermark_free_image/\" + str(idx + 1) + \".jpg\"\n","    watermark_free = Image.open(img_path).convert('RGB')\n","    transform = T.Resize(size = (256,256))\n","\n","    transform_one = T.ToTensor()\n","    img = torch.FloatTensor()\n","    label = torch.FloatTensor()\n","\n","\n","    #img.to(device)\n","    #label.to(device)\n","    img = transform_one(watermarked)\n","    label = transform_one(watermark_free)\n","\n","    img = transform(img)\n","    label = transform(label)\n","\n","\n","    # watermarked.cuda()\n","    # watermark_free.cuda()\n","\n","\n","    return img, label\n","\n","  def __len__(self):\n","    #hardcoded for now\n","    return 60_000\n","\n","Train_dataset = Coles(\"\", \"\")"]},{"cell_type":"code","source":["# Test traing the discriminator - give watermarked and watermark free images from the dataset and have it classify them as either waterrmarked or not\n","\n","\n","class Discrim_dataset(Dataset):\n","  def __init__(self, watermark_free, watermarked, transform = None, target_transform = None):\n","    #self.watermark_free = Image.open(\"CLWD/train/Watermark_image\" + watermark_free)\n","    #self.img_dir = watermarked\n","    self.transform = transform\n","    self.target_transform = target_transform\n","  # C:\\Users\\Cole\\Desktop/CLWD/\n","  def __getitem__(self, idx):\n","    # pick randomly watermarked or not\n","    num = random.randint(0, 1)\n","    if num == 0:\n","      img_path = r\"CLWD/train/Watermarked_image/\" + str(idx + 1) + \".jpg\"\n","      label = 1\n","    else:\n","      img_path = r\"CLWD/train/Watermark_free_image/\" + str(idx + 1) + \".jpg\"\n","      label = 0\n","\n","    image = Image.open(img_path).convert('RGB')\n","    transform = T.Resize(size = (256,256))\n","\n","    transform_one = T.ToTensor()\n","    img = torch.FloatTensor()\n","\n","\n","\n","    #img.to(device)\n","    #label.to(device)\n","    img = transform_one(image)\n","\n","    img = transform(img)\n","\n","    # watermarked.cuda()\n","    # watermark_free.cuda()\n","\n","    return img, label\n","\n","  def __len__(self):\n","    #hardcoded for now\n","    return 60_000\n","\n","discriminator_dataset = Discrim_dataset(\"\", \"\")"],"metadata":{"id":"xJQVQHZGur-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmE6EIhHhOrw"},"outputs":[],"source":["# Defining the model - encoder and decoder convolutional part\n","\n","class Encoder(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    #takes in 64*64 (or as much as I reasonably can) by 3 channels\n","    self.encoder = nn.Sequential(\n","        nn.Conv2d(3, 64, 3, stride = 2, padding = 1),\n","        nn.ReLU(),\n","        nn.Conv2d(64, 128, 3, stride = 2, padding = 1),\n","        nn.ReLU(),\n","        nn.Conv2d(128, 256, 3, stride = 2, padding = 1),\n","        nn.ReLU(),\n","        nn.Conv2d(256, 512, 3, stride = 2, padding = 1),\n","\n","\n","    )\n","        # this may not be perfect but this is how i think it is right now\n","        # conv2d(in_channels, out_channels, kernel size, stride, padding)\n","        # in channels is the number of channels that are input, for this case 3 because R, G, B\n","        # out channels is number of output(width of the matrix?), will be batch size - number of images\n","        # kernel size, area of sqare (or rectangle if tuple is provided) that is used to convolve pixel\n","        # kernel size will probably just be 3, meaning thagt if x is the convoluted pixel then it would take from pixels like this:\n","        # PPP\n","        # PxP\n","        # PPP\n","        # stride is how many pixels it skips, should probably just be 1\n","        # padding is how much it leaves off the edges, should be 1\n","\n","    self.decoder = nn.Sequential(\n","\n","        nn.ConvTranspose2d(512, 256, 3, stride = 2, padding = 1, output_padding = 1),\n","        nn.ReLU(),\n","        nn.ConvTranspose2d(256, 128, 3, stride = 2, padding = 1, output_padding = 1),\n","        nn.ReLU(),\n","        nn.ConvTranspose2d(128, 64, 3, stride = 2, padding = 1, output_padding = 1),\n","        nn.ReLU(),\n","        nn.ConvTranspose2d(64, 3, 3, stride = 2, padding = 1, output_padding = 1),\n","        nn.Sigmoid()\n","\n","    )\n","\n","  # output\n","  def forward(self, x):\n","    #print(\"began\")\n","    encoded = self.encoder(x)\n","    #print(encoded)\n","    decoded = self.decoder(encoded)\n","    return decoded\n"]},{"cell_type":"code","source":["# # CNN model from here, adjsted a bit\n","# # https://medium.com/thecyphy/train-cnn-model-with-pytorch-21dafb918f48\n","\n","# class Discriminator(nn.Module):\n","#   def __init__(self):\n","#     super().__init__()\n","#     self.Network = nn.Sequential(\n","#         nn.Conv2d(3, 8, 2, stride = 2, padding = 1),\n","#         nn.LeakyReLU(0.2),\n","#         nn.Conv2d(8, 16, 2, stride = 2,  padding = 1),\n","\n","\n","#         nn.Conv2d(16, 32, 2, stride = 2,  padding = 1),\n","#         nn.LeakyReLU(0.2),\n","#         nn.Conv2d(32, 64, 2, stride = 2,  padding = 1),\n","\n","#         nn.Conv2d(64, 128, 2, stride = 2,  padding = 1),\n","#         nn.LeakyReLU(0.2),\n","#         nn.Conv2d(128, 1, 2, stride = 2,  padding = 1),\n","#         nn.LeakyReLU(0.2),\n","#         nn.Conv2d(128, 1, 2, stride = 2,  padding = 1),\n","#         nn.Sigmoid(),\n","\n","\n","    )\n","    # self.conv1 = nn.Conv2d(3, 6, 5)\n","    # self.pool = nn.MaxPool2d(2, 2)\n","    # self.conv2 = nn.Conv2d(6, 16, 5)\n","\n","\n","\n","\n","  def forward(self, x):\n","      return self.Network(x)\n"],"metadata":{"id":"UQODeG8MrA_g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_discrim_dataloader = DataLoader(discriminator_dataset, batch_size=16, shuffle=True)\n","# dataiter = iter(train_discrim_dataloader)\n","# images, labels = next(dataiter)\n","# print(images.shape)\n","# var_1 = nn.Conv2d(3, 6, 5)\n","# var_2 = nn.ReLU()\n","# var_3 = nn.Conv2d(6, 16, 3)\n","# var_4 = nn.ReLU()\n","# pool = nn.MaxPool2d(2, 2)\n","# var_5 = nn.Conv2d(16, 32, 5)\n","# var_6 = nn.ReLU()\n","# var_7 = nn.Conv2d(32, 64, 3)\n","\n","\n","# var_9 = nn.Conv2d(64, 128, 5)\n","# var_10 = nn.ReLU()\n","# var_11 = nn.Conv2d(128, 256, 3)\n","# var_13 = nn.Sigmoid()\n","\n","\n","# x = var_1(images)\n","# print(x.shape)\n","# x = var_2(x)\n","# print(x.shape)\n","# x = var_3(x)\n","# print(x.shape)\n","\n","# x = pool(x)\n","# print(x.shape)\n","\n","# x = var_5(x)\n","# print(x.shape)\n","# x = var_6(x)\n","# print(x.shape)\n","# x = var_7(x)\n","# print(x.shape)\n","\n","# x = pool(x)\n","# print(x.shape)\n","\n","\n","# x = var_9(x)\n","# print(x.shape)\n","# x = var_10(x)\n","# print(x.shape)\n","# x = var_11(x)\n","# print(x.shape)\n","# x = pool(x)\n","# print(x.shape)\n","\n","\n","\n"],"metadata":{"id":"7xnot0C4-J95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# testest\n","\n","train_discrim_dataloader = DataLoader(discriminator_dataset, batch_size=16, shuffle=True)\n","dataiter = iter(train_discrim_dataloader)\n","images, labels = next(dataiter)\n","print(images.shape)\n","\n","\n","one = nn.Conv2d(3, 6, 2, stride = 2, padding = 1)\n","two = nn.LeakyReLU(0.2)\n","three = nn.Conv2d(6, 16, 2, stride = 2,  padding = 1)\n","\n","\n","four = nn.Conv2d(16, 32, 2, stride = 2,  padding = 1)\n","five = nn.LeakyReLU(0.2)\n","six = nn.Conv2d(32, 64, 2, stride = 2,  padding = 1)\n","\n","seven = nn.Conv2d(64, 128, 2, stride = 2,  padding = 1)\n","eight = nn.LeakyReLU(0.2)\n","nine = nn.Conv2d(128, 1, 2, stride = 2,  padding = 1)\n","\n","eleven = nn.Flatten(0, -1)\n","twelve = nn.Linear(400, 16)\n","ten = nn.Sigmoid()\n","\n","x = one(images)\n","print(x.shape)\n","x = two(x)\n","print(x.shape)\n","x = three(x)\n","print(x.shape)\n","\n","x = four(x)\n","print(x.shape)\n","x = five(x)\n","print(x.shape)\n","x = six(x)\n","print(x.shape)\n","\n","x = seven(x)\n","print(x.shape)\n","x = eight(x)\n","print(x.shape)\n","x = nine(x)\n","print(x.shape)\n","\n","\n","x = eleven(x)\n","print(x.shape)\n","\n","x = twelve(x)\n","print(x.shape)\n","\n","x = ten(x)\n","print(x.shape)\n","print(x)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pH2ZXjt50drO","executionInfo":{"status":"ok","timestamp":1686715071351,"user_tz":420,"elapsed":74,"user":{"displayName":"FlyPengiun 44","userId":"12676730952236288290"}},"outputId":"3f5f664f-f04e-41f4-afac-7a4e0f22befb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 3, 256, 256])\n","torch.Size([16, 6, 129, 129])\n","torch.Size([16, 6, 129, 129])\n","torch.Size([16, 16, 65, 65])\n","torch.Size([16, 32, 33, 33])\n","torch.Size([16, 32, 33, 33])\n","torch.Size([16, 64, 17, 17])\n","torch.Size([16, 128, 9, 9])\n","torch.Size([16, 128, 9, 9])\n","torch.Size([16, 1, 5, 5])\n","torch.Size([400])\n","torch.Size([16])\n","torch.Size([16])\n","tensor([0.4990, 0.4854, 0.4916, 0.4978, 0.4995, 0.5085, 0.5009, 0.5157, 0.4997,\n","        0.5086, 0.4973, 0.5045, 0.4908, 0.5172, 0.4993, 0.4853],\n","       grad_fn=<SigmoidBackward0>)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysmRxfFEvdjF","executionInfo":{"status":"error","timestamp":1686679584627,"user_tz":240,"elapsed":326,"user":{"displayName":"FlyPengiun 44","userId":"12676730952236288290"}},"colab":{"base_uri":"https://localhost:8080/","height":236},"outputId":"6c415919-5f4c-4b0c-8d81-df57d3bcde45"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-f41fc371c3c7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initializing the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdiscrim_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrim_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Discriminator' is not defined"]}],"source":["# # initializing the model\n","# model = Encoder()\n","# discrim_model = Discriminator()\n","# criterion = nn.MSELoss()\n","# optimizer = torch.optim.SGD(discrim_model.parameters(), lr = 1e-3)\n"]},{"cell_type":"code","source":["# CNN model from here, adjsted a bit\n","# https://medium.com/thecyphy/train-cnn-model-with-pytorch-21dafb918f48\n","\n","class Discriminator(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.Network = nn.Sequential(\n","        nn.Conv2d(3, 8, 2, stride = 2, padding = 1),\n","        nn.LeakyReLU(0.2),\n","        nn.Conv2d(8, 16, 2, stride = 2,  padding = 1),\n","        nn.BatchNorm2d(16),\n","\n","        nn.Conv2d(16, 32, 2, stride = 2,  padding = 1),\n","        nn.LeakyReLU(0.2),\n","        nn.Conv2d(32, 64, 2, stride = 2,  padding = 1),\n","        nn.BatchNorm2d(64),\n","\n","        nn.Conv2d(64, 128, 2, stride = 2,  padding = 1),\n","        nn.LeakyReLU(0.2),\n","        nn.Conv2d(128, 1, 2, stride = 2,  padding = 1),\n","        nn.BatchNorm2d(1),\n","\n","        nn.Flatten(0, -1),\n","        nn.Linear(400, 16),\n","        nn.Sigmoid()\n","\n","\n","    )\n","    # self.conv1 = nn.Conv2d(3, 6, 5)\n","    # self.pool = nn.MaxPool2d(2, 2)\n","    # self.conv2 = nn.Conv2d(6, 16, 5)\n","\n","\n","\n","\n","  def forward(self, x):\n","\n","      return self.Network(x)\n","\n","# initializing the model\n","model = Encoder()\n","discrim_model = Discriminator()\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(discrim_model.parameters(), lr = 1e-2)\n","\n","# practice training loop for the discriminator\n","train_discrim_dataloader = DataLoader(discriminator_dataset, batch_size=16, shuffle=True, num_workers = 8)\n","\n","discrim_model.to(device)\n","model.to(device)\n","criterion.to(device)\n","#optimizer.to(device)\n","# stuffs = T.Resize(size = (256,256))\n","# optimus_prime = T.ToPILImage()\n","# pic123 = Image.open(\"Downloads/images.jpg\").convert('RGB')\n","# transform_three = T.ToTensor()\n","# img = transform_three(pic123)\n","# img = torch.FloatTensor(img)\n","# img = stuffs(img)\n","# img = img.to(device)\n","\n","# print(img.shape)\n","\n","#transform = T.Resize(size = (256,256))\n","transform = T.ToPILImage()\n","\n","# Training loop\n","epochs = 12\n","for e in range(epochs):\n","    counter = 0\n","    running_loss = 0\n","    for images, labels in train_discrim_dataloader:\n","        counter += 1\n","        if counter % 37.5 == 0:\n","          print(\"training at epoch \", e, \" \", (counter / 3750) * 100, \"% done\")\n","        # print(images.shape)\n","        # images = images.reshape(-1, 256*256) * 0.00390625\n","        # print(images.shape)\n","        # print(images)\n","        # images = images.float()\n","        # print(images.shape)\n","        # print(images)\n","        labels = labels.to(device)\n","\n","        #print(len(images))\n","        #batch_size, channels, depth, height, width\n","\n","        #images = np.reshape(images, (len(images), 3, 256, 256)) /256\n","\n","        images = images.to(device)\n","        #images = torch.cat([images[0:-1], images[-1+1:]])\n","\n","\n","        #images = np.reshape(images, (len(images), 3, 256, 256)) /256\n","\n","\n","        #images = images.float()\n","        #print(images)\n","        #print(images.shape)\n","\n","        # Training pass\n","\n","        #print(type(images))\n","        # test123 = labels[0]\n","        # pic2 = transform(test123)\n","        # pic2.show()\n","\n","        # test1234 = images[0]\n","        # pic3 = transform(test1234)\n","        # pic3.show()\n","        #print(images.shape)\n","        output = discrim_model(images)\n","        #print(output.shape)\n","\n","\n","        output.to(device)\n","        #print(output.shape)\n","        #print(labels.shape)\n","        #print(output)\n","        #print(labels)\n","        output = output.float()\n","        labels = labels.float()\n","\n","\n","        optimizer.zero_grad()\n","        #log_softmax = nn.LogSoftmax(dim = -1)\n","        #print(log_softmax(output).shape)\n","        #print(\"output cuda: \", output.is_cuda)\n","        #print(\"labels cuda: \", labels.is_cuda)\n","        loss = criterion(output, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","\n","\n","    # output = discrim_model(img)\n","    # output = output * 255\n","    # pic1 = transform(output)\n","    # pic1.show()\n","    torch.save({\n","            'epoch': e,\n","            'model_state_dict': discrim_model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            }, f\"discrim_4_epoch-{e}.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"u440QfwhxGPk","outputId":"dfb4210e-34a0-41fa-951b-09f3ab6c651c","executionInfo":{"status":"error","timestamp":1686765811994,"user_tz":240,"elapsed":1073427,"user":{"displayName":"FlyPengiun 44","userId":"12676730952236288290"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["training at epoch  0   2.0 % done\n","training at epoch  0   4.0 % done\n","training at epoch  0   6.0 % done\n","training at epoch  0   8.0 % done\n","training at epoch  0   10.0 % done\n","training at epoch  0   12.0 % done\n","training at epoch  0   14.000000000000002 % done\n","training at epoch  0   16.0 % done\n","training at epoch  0   18.0 % done\n","training at epoch  0   20.0 % done\n","training at epoch  0   22.0 % done\n","training at epoch  0   24.0 % done\n","training at epoch  0   26.0 % done\n","training at epoch  0   28.000000000000004 % done\n","training at epoch  0   30.0 % done\n","training at epoch  0   32.0 % done\n","training at epoch  0   34.0 % done\n","training at epoch  0   36.0 % done\n","training at epoch  0   38.0 % done\n","training at epoch  0   40.0 % done\n","training at epoch  0   42.0 % done\n","training at epoch  0   44.0 % done\n","training at epoch  0   46.0 % done\n","training at epoch  0   48.0 % done\n","training at epoch  0   50.0 % done\n","training at epoch  0   52.0 % done\n","training at epoch  0   54.0 % done\n","training at epoch  0   56.00000000000001 % done\n","training at epoch  0   57.99999999999999 % done\n","training at epoch  0   60.0 % done\n","training at epoch  0   62.0 % done\n","training at epoch  0   64.0 % done\n","training at epoch  0   66.0 % done\n","training at epoch  0   68.0 % done\n","training at epoch  0   70.0 % done\n","training at epoch  0   72.0 % done\n","training at epoch  0   74.0 % done\n","training at epoch  0   76.0 % done\n","training at epoch  0   78.0 % done\n","training at epoch  0   80.0 % done\n","training at epoch  0   82.0 % done\n","training at epoch  0   84.0 % done\n","training at epoch  0   86.0 % done\n","training at epoch  0   88.0 % done\n","training at epoch  0   90.0 % done\n","training at epoch  0   92.0 % done\n","training at epoch  0   94.0 % done\n","training at epoch  0   96.0 % done\n","training at epoch  0   98.0 % done\n","training at epoch  0   100.0 % done\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["training at epoch  1   2.0 % done\n","training at epoch  1   4.0 % done\n","training at epoch  1   6.0 % done\n","training at epoch  1   8.0 % done\n","training at epoch  1   10.0 % done\n","training at epoch  1   12.0 % done\n","training at epoch  1   14.000000000000002 % done\n","training at epoch  1   16.0 % done\n","training at epoch  1   18.0 % done\n","training at epoch  1   20.0 % done\n","training at epoch  1   22.0 % done\n","training at epoch  1   24.0 % done\n","training at epoch  1   26.0 % done\n","training at epoch  1   28.000000000000004 % done\n","training at epoch  1   30.0 % done\n","training at epoch  1   32.0 % done\n","training at epoch  1   34.0 % done\n","training at epoch  1   36.0 % done\n","training at epoch  1   38.0 % done\n","training at epoch  1   40.0 % done\n","training at epoch  1   42.0 % done\n","training at epoch  1   44.0 % done\n","training at epoch  1   46.0 % done\n","training at epoch  1   48.0 % done\n","training at epoch  1   50.0 % done\n","training at epoch  1   52.0 % done\n","training at epoch  1   54.0 % done\n","training at epoch  1   56.00000000000001 % done\n","training at epoch  1   57.99999999999999 % done\n","training at epoch  1   60.0 % done\n","training at epoch  1   62.0 % done\n","training at epoch  1   64.0 % done\n","training at epoch  1   66.0 % done\n","training at epoch  1   68.0 % done\n","training at epoch  1   70.0 % done\n","training at epoch  1   72.0 % done\n","training at epoch  1   74.0 % done\n","training at epoch  1   76.0 % done\n","training at epoch  1   78.0 % done\n","training at epoch  1   80.0 % done\n","training at epoch  1   82.0 % done\n","training at epoch  1   84.0 % done\n","training at epoch  1   86.0 % done\n","training at epoch  1   88.0 % done\n","training at epoch  1   90.0 % done\n","training at epoch  1   92.0 % done\n","training at epoch  1   94.0 % done\n","training at epoch  1   96.0 % done\n","training at epoch  1   98.0 % done\n","training at epoch  1   100.0 % done\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["training at epoch  2   2.0 % done\n","training at epoch  2   4.0 % done\n","training at epoch  2   6.0 % done\n","training at epoch  2   8.0 % done\n","training at epoch  2   10.0 % done\n","training at epoch  2   12.0 % done\n","training at epoch  2   14.000000000000002 % done\n","training at epoch  2   16.0 % done\n","training at epoch  2   18.0 % done\n","training at epoch  2   20.0 % done\n","training at epoch  2   22.0 % done\n","training at epoch  2   24.0 % done\n","training at epoch  2   26.0 % done\n","training at epoch  2   28.000000000000004 % done\n","training at epoch  2   30.0 % done\n","training at epoch  2   32.0 % done\n","training at epoch  2   34.0 % done\n","training at epoch  2   36.0 % done\n","training at epoch  2   38.0 % done\n","training at epoch  2   40.0 % done\n","training at epoch  2   42.0 % done\n","training at epoch  2   44.0 % done\n","training at epoch  2   46.0 % done\n","training at epoch  2   48.0 % done\n","training at epoch  2   50.0 % done\n","training at epoch  2   52.0 % done\n","training at epoch  2   54.0 % done\n","training at epoch  2   56.00000000000001 % done\n","training at epoch  2   57.99999999999999 % done\n","training at epoch  2   60.0 % done\n","training at epoch  2   62.0 % done\n","training at epoch  2   64.0 % done\n","training at epoch  2   66.0 % done\n","training at epoch  2   68.0 % done\n","training at epoch  2   70.0 % done\n","training at epoch  2   72.0 % done\n","training at epoch  2   74.0 % done\n","training at epoch  2   76.0 % done\n","training at epoch  2   78.0 % done\n","training at epoch  2   80.0 % done\n","training at epoch  2   82.0 % done\n","training at epoch  2   84.0 % done\n","training at epoch  2   86.0 % done\n","training at epoch  2   88.0 % done\n","training at epoch  2   90.0 % done\n","training at epoch  2   92.0 % done\n","training at epoch  2   94.0 % done\n","training at epoch  2   96.0 % done\n","training at epoch  2   98.0 % done\n","training at epoch  2   100.0 % done\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["training at epoch  3   2.0 % done\n","training at epoch  3   4.0 % done\n","training at epoch  3   6.0 % done\n","training at epoch  3   8.0 % done\n","training at epoch  3   10.0 % done\n","training at epoch  3   12.0 % done\n","training at epoch  3   14.000000000000002 % done\n","training at epoch  3   16.0 % done\n","training at epoch  3   18.0 % done\n","training at epoch  3   20.0 % done\n","training at epoch  3   22.0 % done\n","training at epoch  3   24.0 % done\n","training at epoch  3   26.0 % done\n","training at epoch  3   28.000000000000004 % done\n","training at epoch  3   30.0 % done\n","training at epoch  3   32.0 % done\n","training at epoch  3   34.0 % done\n","training at epoch  3   36.0 % done\n","training at epoch  3   38.0 % done\n","training at epoch  3   40.0 % done\n","training at epoch  3   42.0 % done\n","training at epoch  3   44.0 % done\n","training at epoch  3   46.0 % done\n","training at epoch  3   48.0 % done\n","training at epoch  3   50.0 % done\n","training at epoch  3   52.0 % done\n","training at epoch  3   54.0 % done\n","training at epoch  3   56.00000000000001 % done\n","training at epoch  3   57.99999999999999 % done\n","training at epoch  3   60.0 % done\n","training at epoch  3   62.0 % done\n","training at epoch  3   64.0 % done\n","training at epoch  3   66.0 % done\n","training at epoch  3   68.0 % done\n","training at epoch  3   70.0 % done\n","training at epoch  3   72.0 % done\n","training at epoch  3   74.0 % done\n","training at epoch  3   76.0 % done\n","training at epoch  3   78.0 % done\n","training at epoch  3   80.0 % done\n","training at epoch  3   82.0 % done\n","training at epoch  3   84.0 % done\n","training at epoch  3   86.0 % done\n","training at epoch  3   88.0 % done\n","training at epoch  3   90.0 % done\n","training at epoch  3   92.0 % done\n","training at epoch  3   94.0 % done\n","training at epoch  3   96.0 % done\n","training at epoch  3   98.0 % done\n","training at epoch  3   100.0 % done\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["training at epoch  4   2.0 % done\n","training at epoch  4   4.0 % done\n","training at epoch  4   6.0 % done\n","training at epoch  4   8.0 % done\n","training at epoch  4   10.0 % done\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c97e4fe94424>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m#print(\"labels cuda: \", labels.is_cuda)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGjVlEte5_uB","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"805bc82f-606c-4cf4-a463-8a6948cc1357","executionInfo":{"status":"error","timestamp":1686715531694,"user_tz":420,"elapsed":13604,"user":{"displayName":"FlyPengiun 44","userId":"12676730952236288290"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["training at epoch  0   2.0 % done\n","training at epoch  0   4.0 % done\n","training at epoch  0   6.0 % done\n","training at epoch  0   8.0 % done\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[41], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     19\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     21\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m37.5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[1;32mIn[39], line 21\u001b[0m, in \u001b[0;36mDiscrim_dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m   img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCole\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop/CLWD/CLWD/train/Watermark_free_image/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m   label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 21\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mResize(size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m))\n\u001b[0;32m     24\u001b[0m transform_one \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mToTensor()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    875\u001b[0m ):\n\u001b[0;32m    876\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\ImageFile.py:242\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 242\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\JpegImagePlugin.py:403\u001b[0m, in \u001b[0;36mJpegImageFile.load_read\u001b[1;34m(self, read_bytes)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes):\n\u001b[0;32m    398\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m    internal: read more image data\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile\u001b[38;5;241m.\u001b[39mLOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ended\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# # initializing model object and dataset for training\n","\n","# train_dataloader = DataLoader(discriminator_dataset, batch_size=16, shuffle=True)\n","\n","# # move to gpu - im not gonna put this in yet until all other bugs are worked out\n","# # so as to not intrudce more until current ones are fixed\n","# #discrim_model.to(device)\n","# criterion.to(device)\n","# #optimizer.to(device)\n","\n","\n","# #transform = T.Resize(size = (256,256))\n","# transform = T.ToPILImage()\n","\n","# # Training loop\n","# epochs = 12\n","# for e in range(epochs):\n","#     counter = 0\n","#     running_loss = 0\n","#     for images, labels in train_dataloader:\n","#         counter += 1\n","#         if counter % 37.5 == 0:\n","#           print(\"training at epoch \", e, \" \", (counter / 3750) * 100, \"% done\")\n","#         labels = labels.to(device)\n","#         # print(images.shape)\n","#         # images = images.reshape(-1, 256*256) * 0.00390625\n","#         # print(images.shape)\n","#         # print(images)\n","#         # images = images.float()\n","#         # print(images.shape)\n","#         # print(images)\n","\n","#         #print(len(images))\n","#         #batch_size, channels, depth, height, width\n","\n","#         #images = np.reshape(images, (len(images), 3, 256, 256)) /256\n","\n","#         images = images.to(device)\n","#         #images = torch.cat([images[0:-1], images[-1+1:]])\n","\n","\n","#         #images = np.reshape(images, (len(images), 3, 256, 256)) /256\n","\n","#         #print(images.shape)\n","#         #images = images.float()\n","#         #print(images)\n","#         #print(images.shape)\n","\n","#         # Training pass\n","\n","#         #print(type(images))\n","#         # test123 = labels[0]\n","#         # pic2 = transform(test123)\n","#         # pic2.show()\n","\n","#         # test1234 = images[0]\n","#         # pic3 = transform(test1234)\n","#         # pic3.show()\n","\n","#         output = discrim_model(images)\n","\n","#         output.to(device)\n","#         output = output.float()\n","#         labels = labels.float()\n","\n","#         optimizer.zero_grad()\n","\n","#         #print(\"output cuda: \", output.is_cuda)\n","#         #print(\"labels cuda: \", labels.is_cuda)\n","#         loss = criterion(output, labels)\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         running_loss += loss.item()\n","\n","#     transform = T.ToPILImage()\n","#     pic1 = transform(img)\n","#     pic1.show()\n","#     torch.save({\n","#             'epoch': e,\n","#             'model_state_dict': discrim_model.state_dict(),\n","#             'optimizer_state_dict': optimizer.state_dict(),\n","#             'loss': loss,\n","#             }, f\"discrim_1_epoch-{e}.pth\")"]},{"cell_type":"code","source":["# Display image and label.\n","\n","# testing dataset\n","train_dataloader = DataLoader(discriminator_dataset, batch_size=16, shuffle=True)\n","\n","train_features, train_labels = next(iter(train_dataloader))\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","img = train_features[0]#.squeeze()\n","label = train_labels[0]\n","\n","transform = T.ToPILImage()\n","pic1 = transform(img)\n","pic1.show()\n","\n","# label = train_features[0]#.squeeze()\n","# label = train_labels[0]\n","# pic2 = transform(label)\n","# pic2.show()\n","\n","print(f\"Label: {label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ar_nQH4grr9u","executionInfo":{"status":"ok","timestamp":1686502361787,"user_tz":420,"elapsed":3241,"user":{"displayName":"FlyPengiun 44","userId":"12676730952236288290"}},"outputId":"4a9c4bf1-59fe-4a20-d566-716a4ecf88e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature batch shape: torch.Size([16, 3, 150, 150])\n","Labels batch shape: torch.Size([16])\n","Label: 0\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1liQNZlLcYOeSqgkJgKSPMAr25mf4bnwA","timestamp":1686159042164}],"gpuType":"T4","mount_file_id":"1liQNZlLcYOeSqgkJgKSPMAr25mf4bnwA","authorship_tag":"ABX9TyN4Uno66qzLCMobpAaCDGtG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}